# DNN training and compression

### Folders: 

- **src folder:** contains source codes that correspond to auxiliary functions.
- **tunerStudent folder**: a temporal folder will be created to store intermediate results generated by the tuner to obtain the hyperparameters defining the student architecture.
- **tunerTeacher folder**: a temporal folder will be created to store intermediate results generated by the tuner to obtain the hyperparameters defining the teacher architecture.

### Datasets

- Custom datasets will be provided under reasonable request.
- The methodology can be implemented using SOTA datasets, such as MNIST and CIFAR


### Files:

- **config.py**: a configuration file to set different variables that are part of the process of the DNN training and compression. The user should modify this file based on his specifications.

- **compressionMain.ipynb**: main Jupyter Notebook file to perform model training and compression.

- **compressionStart.py**: a file that contains the selection flow based on the type of signal (1D or 2D).

- Several **templates** are located in the folder **topology** and are provided to define the network topologies and to perform the hyperparameters optimization and training:
    - For 1D signals: topologies for teacher, and student are provided (for both, hyperparameters optimization and final model training). 
      
    - For 2D signals: topologies for teacher, and student are provided (for both, hyperparameters optimization and final model training).
 
      The developer can modify these files to adapt the workflow to his needs.

### How to start? 

The file **compressionMain.ipynb** has the main function for performing hyperparameters search, training, and compression, including: 
    - Load dataset 
   
    - Hyperparameters optimization for teacher topology.

    - Training of the teacher model.
    
    - Hyperparameters optimization for the student topology (combined with pruning, quantization, and knowledge distillation.
    
    - Training of the student model (combined with pruning, quantization, and knowledge distillation).
    
    - Metric report.
    
    - Teacher and student models saved in the folder **models**.

### Final remarks

**Have fun!!** 
And remember, this is a methodology to facilitate the training and compression process when targetting resource-constrained devices, it is not (yet ;) ) an automatic process.
